model: claude:claude-3-5-sonnet-20240620 # Default model
temperature: null # Set default temperature parameter
top_p: null # Set default top-p parameter
save: false # Indicates whether to persist the message
save_session: null # Controls the persistence of the session, if null, asking the user
highlight: true # Controls syntax highlighting
light_theme: false # Activates a light color theme when true. ENV: AICHAT_LIGHT_THEME
wrap: auto # Controls text wrapping (no, auto, <max-width>)
wrap_code: false # Enables or disables wrapping of code blocks
auto_copy: false # Enables or disables automatic copying the last LLM response to the clipboard
keybindings: emacs # Choose keybinding style (emacs, vi)
prelude: null # Set a default role or session to start with (role:<name>, session:<name>)

# Command that will be used to edit the current line buffer with ctrl+o
# if unset fallback to $EDITOR and $VISUAL
buffer_editor: null
# Compress session when token count reaches or exceeds this threshold (must be at least 1000)
compress_threshold: 1600
# Text prompt used for creating a concise summary of session message
summarize_prompt: "Summarize the discussion briefly in 200 words or less to use as a prompt for future context."
# Text prompt used for including the summary of the entire session
summary_prompt: "This is a summary of the chat history as a recap: "

# Custom REPL prompt, see https://github.com/sigoden/aichat/wiki/Custom-REPL-Prompt
left_prompt: "{color.green}{?session {session}{?role /}}{role}{color.cyan}{?session )}{!session >}{color.reset} "
right_prompt: "{color.purple}{?session {?consume_tokens {consume_tokens}({consume_percent}%)}{!consume_tokens {consume_tokens}}}{color.reset}"

clients:
  # All clients have the following configuration:
  # - type: xxxx
  #   name: xxxx                                      # Only use it to distinguish clients with the same client type. Optional
  #   models:
  #     - name: xxxx                                  # The model name
  #       max_input_tokens: 100000
  #       max_output_tokens: 4096
  #       supports_vision: true
  #       extra_fields:                               # Set custom parameters, will merge with the body json
  #          key: value
  #   extra:
  #     proxy: socks5://127.0.0.1:1080                # Specify https/socks5 proxy server. ENV: HTTPS_PROXY/ALL_PROXY
  #     connect_timeout: 10                           # Set a timeout in seconds for connect to server

  # OpenAI
  # See https://platform.openai.com/docs/quickstart
  - type: openai

  # Claude
  # See https://docs.anthropic.com/claude/reference/getting-started-with-the-api
  - type: claude

  # Openrouter
  # See https://openrouter.ai/docs#quick-start
  - type: openai-compatible
    name: openrouter
    api_base: https://openrouter.ai/api/v1

  - type: openai-compatible
    name: local
    api_base: http://localhost:8080/v1 # ENV: {client}_API_BASE
    chat_endpoint: /chat/completions # Optional
    models:
      - name: model

  # # Ollama
  # # See https://github.com/jmorganca/ollama
  # - type: ollama
  #   api_base: http://localhost:11434                  # ENV: {client}_API_BASE
  #   models:                                           # Required
  #     - name: llama3:8b-instruct-q8_0
  #       max_input_tokens: 8192
