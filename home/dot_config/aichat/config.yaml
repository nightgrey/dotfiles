# https://github.com/sigoden/aichat/blob/main/config.example.yaml
# https://github.com/sigoden/aichat/wiki/Configuration-Guide
# ---- llm ----
model: openrouter:anthropic/claude-sonnet-4  # Specify the LLM to use
temperature: 0.2                # Set default temperature parameter
top_p: null                      # Set default top-p parameter, range (0, 1)

# ---- behavior ----
save: true                       # Indicates whether to persist the message
editor: nano                     # Specifies the command used to edit input buffer or session. (e.g. vim, emacs, nano).
wrap: auto                       # Controls text wrapping (no, auto, <max-width>)
wrap_code: true                 # Enables or disables wrapping of code blocks
save_shell_history: true         # Save shell history

# ---- session ----
# Controls the persistence of the session. if true, auto save; if false, not save; if null, asking the user
save_session: true

clients:
  # All clients have the following configuration:
  # - type: xxxx
  #   name: xxxx                                      # Only use it to distinguish clients with the same client type. Optional
  #   models:
  #     - name: xxxx                                  # The model name
  #       max_input_tokens: 100000
  #       max_output_tokens: 4096
  #       supports_vision: true
  #       extra_fields:                               # Set custom parameters, will merge with the body json
  #          key: value
  #   extra:
  #     proxy: socks5://127.0.0.1:1080                # Specify https/socks5 proxy server. ENV: HTTPS_PROXY/ALL_PROXY
  #     connect_timeout: 10                           # Set a timeout in seconds for connect to server

  # OpenAI
  # See https://platform.openai.com/docs/quickstart
  - type: openai

  # Claude
  # See https://docs.anthropic.com/claude/reference/getting-started-with-the-api
  - type: claude

  # Openrouter
  # See https://openrouter.ai/docs#quick-start
  - type: openai-compatible
    name: openrouter
    api_base: https://openrouter.ai/api/v1
    
  - type: openai-compatible
    name: hyperbolic
    api_base: https://api.hyperbolic.xyz/v1
    chat_endpoint: /chat/completions
    models:
      - name: meta-llama/Meta-Llama-3.1-405B-Instruct
        max_input_tokens: 128000
        supports_vision: false
        supports_function_calling: false

      - name: meta-llama/Meta-Llama-3.1-70B-Instruct
        max_input_tokens: 128000
        supports_vision: false
        supports_function_calling: false

  - type: openai-compatible
    name: local
    api_base: http://localhost:8080/v1 # ENV: {client}_API_BASE
    chat_endpoint: /chat/completions # Optional
    api_key: ${HYPERBOLIC_API_KEY}
    models:
      - name: model
